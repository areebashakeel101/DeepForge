{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwoqzeeLYXeH",
        "outputId": "db00b537-690c-4066-e017-078f972bde1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ZIP_PATH = \"/content/drive/MyDrive/captions.json.zip\"\n",
        "!unzip -q \"{ZIP_PATH}\" -d \"{EXTRACT_TO}\""
      ],
      "metadata": {
        "id": "yH29nRNWY0mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/captions.json.zip\"\n",
        "!unzip -q \"{ZIP_PATH}\" -d \"{EXTRACT_TO}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRKCeR6iZLPl",
        "outputId": "a994ef60-330c-4ee8-8640-34cd1ed8b602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/captions.json.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EXTRACT_TO = \"/content/mini_coco\""
      ],
      "metadata": {
        "id": "31rmplHnZUMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/mini_coco\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcWO4-BwZecW",
        "outputId": "d2b7e435-a88b-4d18-a9b8-d06ba141a3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "captions.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8624ea60"
      },
      "source": [
        "# Task\n",
        "Prepare `image_features_train`, `input_sequences_train`, and `target_sequences_train` arrays by using the extracted MobileNet image features and the processed caption sequences, padding the caption sequences to `max_caption_length`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6394c9"
      },
      "source": [
        "## Load and Preprocess Captions from captions.json\n",
        "\n",
        "### Subtask:\n",
        "Load the `captions.json` file, parse it to extract image IDs and their associated captions, clean the text by lowercasing, removing punctuation, and adding start/end tokens. Then, initialize and fit a Keras Tokenizer on these cleaned captions to build the vocabulary and generate numerical sequences. Finally, determine the `max_caption_length` and `vocab_size` from this process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84b4b5c5"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to import necessary libraries, define the path to the captions file, and load the JSON data into a Python dictionary. I'll also initialize a dictionary to store image IDs and their captions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6c9d903",
        "outputId": "cd6d6031-cbe6-492a-fc91-c12715650d5d"
      },
      "source": [
        "import json\n",
        "import string\n",
        "\n",
        "CAPTIONS_FILE_PATH = f\"{EXTRACT_TO}/captions.json\"\n",
        "\n",
        "# Load the captions data\n",
        "with open(CAPTIONS_FILE_PATH, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Initialize a dictionary to store image IDs and their captions\n",
        "image_captions = {}\n",
        "for entry in data:\n",
        "    image_id = entry['image_id']\n",
        "    caption = entry['caption']\n",
        "    if image_id not in image_captions:\n",
        "        image_captions[image_id] = []\n",
        "    image_captions[image_id].append(caption)\n",
        "\n",
        "print(f\"Loaded {len(data)} caption entries.\")\n",
        "print(f\"Extracted captions for {len(image_captions)} unique images.\")\n",
        "print(\"First 5 image_captions entries:\")\n",
        "for i, (img_id, caps) in enumerate(image_captions.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"{img_id}: {caps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 93950 caption entries.\n",
            "Extracted captions for 18783 unique images.\n",
            "First 5 image_captions entries:\n",
            "299675: ['A white square kitchen with tile floor that needs repairs ', 'A white kitchen in a home with the light on.', 'A kitchen with a tile floor has cabinets with no doors, a dishwasher, a sink, and a refrigerator.', 'A white kitchen without doors on the cabinets.', 'A white kitchen filled with pots, pans and dishes.']\n",
            "513461: ['A surfer, a woman, and a child walk on the beach.', 'A person holding a surf board on a beach. ', 'A family standing next to the ocean on a sandy beach with a surf board.', 'A man getting ready to surf as lookers walk by', 'A family carrying a surfboard to the ocean.']\n",
            "18691: ['A few people sit on a dim transportation system. ', 'six people on a bus at night time', 'random people sitting in a public tranport bus.', 'There are only a few people in the bus.', 'A group of people are riding the bus.']\n",
            "285579: ['A person, protected from the rain by their umbrella, walks down the road.', 'A person walking outside in the rain with a green umbrella. ', 'A student walks in the rain with a green umbrella.', 'An individual covers himself under an umbrella on a rainy day.', 'A man walking in the rain crossing a street while holding an umbrella.']\n",
            "266366: ['The bathroom with a toilet has an interesting sink.', 'A demonstration of a well maintained hotel bathroom', 'A clean, mediocre motel bathroom with a nice sink.', 'A modern hotel bathroom with accessories set out.', 'A bathroom with a toilet, sink, mirror and shower stall.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "425e4b54"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the captions are loaded, the next step is to clean them by converting to lowercase, removing punctuation and numbers, and adding 'startseq' and 'endseq' tokens. These cleaned captions will be stored in `cleaned_captions_list`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17c1a49e",
        "outputId": "dce9ef5b-dccf-42b3-f3ca-2e269e53aea7"
      },
      "source": [
        "cleaned_captions_list = []\n",
        "\n",
        "for image_id, captions in image_captions.items():\n",
        "    for caption in captions:\n",
        "        # Convert to lowercase\n",
        "        caption = caption.lower()\n",
        "        # Remove punctuation and numbers\n",
        "        caption = caption.translate(str.maketrans('', '', string.punctuation))\n",
        "        caption = ''.join([char for char in caption if not char.isdigit()])\n",
        "        # Remove extra spaces\n",
        "        caption = ' '.join(caption.split())\n",
        "        # Add start and end tokens\n",
        "        cleaned_caption = f\"startseq {caption} endseq\"\n",
        "        cleaned_captions_list.append(cleaned_caption)\n",
        "\n",
        "print(f\"Total cleaned captions: {len(cleaned_captions_list)}\")\n",
        "print(\"First 5 cleaned captions:\")\n",
        "for i, cap in enumerate(cleaned_captions_list[:5]):\n",
        "    print(f\"{i+1}: {cap}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total cleaned captions: 93950\n",
            "First 5 cleaned captions:\n",
            "1: startseq a white square kitchen with tile floor that needs repairs endseq\n",
            "2: startseq a white kitchen in a home with the light on endseq\n",
            "3: startseq a kitchen with a tile floor has cabinets with no doors a dishwasher a sink and a refrigerator endseq\n",
            "4: startseq a white kitchen without doors on the cabinets endseq\n",
            "5: startseq a white kitchen filled with pots pans and dishes endseq\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9752f75f"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the captions are cleaned, I will import the `Tokenizer` from Keras, instantiate it with an out-of-vocabulary token, fit it on the `cleaned_captions_list` to build the vocabulary, generate numerical sequences, and then calculate `max_caption_length` and `vocab_size`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59e599f5",
        "outputId": "788b46b9-3fba-4ba5-a58b-2eecdc6a6102"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Instantiate a Tokenizer object\n",
        "tokenizer = Tokenizer(oov_token='<unk>')\n",
        "\n",
        "# Fit the tokenizer on the cleaned captions list to build the vocabulary\n",
        "tokenizer.fit_on_texts(cleaned_captions_list)\n",
        "\n",
        "# Generate numerical sequences for each cleaned caption\n",
        "caption_sequences = tokenizer.texts_to_sequences(cleaned_captions_list)\n",
        "\n",
        "# Calculate max_caption_length\n",
        "max_caption_length = max(len(seq) for seq in caption_sequences)\n",
        "\n",
        "# Determine vocab_size (add 1 for padding token)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(f\"Maximum caption length: {max_caption_length}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(\"First 5 caption sequences:\")\n",
        "for i, seq in enumerate(caption_sequences[:5]):\n",
        "    print(f\"{i+1}: {seq}\")\n",
        "print(\"First 10 words in vocabulary:\")\n",
        "# Get words by index to show top words, excluding oov_token if it's not explicitly in word_index at index 1\n",
        "sorted_word_index = sorted(tokenizer.word_index.items(), key=lambda item: item[1])\n",
        "for word, index in sorted_word_index[:10]:\n",
        "    print(f\"{word}: {index}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum caption length: 51\n",
            "Vocabulary size: 12844\n",
            "First 5 caption sequences:\n",
            "1: [3, 2, 22, 669, 65, 9, 674, 205, 26, 1934, 3287, 4]\n",
            "2: [3, 2, 22, 65, 8, 2, 346, 9, 7, 170, 5, 4]\n",
            "3: [3, 2, 65, 9, 2, 674, 205, 54, 456, 9, 473, 843, 2, 1495, 2, 121, 10, 2, 262, 4]\n",
            "4: [3, 2, 22, 65, 1241, 843, 5, 7, 456, 4]\n",
            "5: [3, 2, 22, 65, 152, 9, 968, 1187, 10, 719, 4]\n",
            "First 10 words in vocabulary:\n",
            "<unk>: 1\n",
            "a: 2\n",
            "startseq: 3\n",
            "endseq: 4\n",
            "on: 5\n",
            "of: 6\n",
            "the: 7\n",
            "in: 8\n",
            "with: 9\n",
            "and: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32826e5c"
      },
      "source": [
        "## Extract Image Features with AlexNet and MobileNet\n",
        "\n",
        "### Subtask:\n",
        "Simulate the extraction of MobileNetV2 image features for all unique images, creating a dummy `image_features_train` array. Additionally, simulate AlexNet feature extraction for a single image for dimension comparison, given the absence of actual image files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85d1c2cd"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import numpy, determine the number of unique images, and then create dummy image feature arrays for MobileNetV2 and AlexNet using random values, printing their shapes for verification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e17df948",
        "outputId": "b5feb42c-d608-43f4-8535-3918716192e9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Determine the number of unique images\n",
        "num_unique_images = len(image_captions)\n",
        "\n",
        "# Create a dummy image_features_train array for MobileNetV2\n",
        "# mobilenet_feature_shape is 1280 from kernel state\n",
        "image_features_train = np.random.rand(num_unique_images, mobilenet_feature_shape)\n",
        "\n",
        "# Create a dummy AlexNet feature array for a single image for comparison\n",
        "# alexnet_feature_shape is 9216 from kernel state\n",
        "alexnet_dummy_feature = np.random.rand(1, alexnet_feature_shape)\n",
        "\n",
        "print(f\"Shape of dummy MobileNetV2 features (image_features_train): {image_features_train.shape}\")\n",
        "print(f\"Shape of dummy AlexNet feature for a single image: {alexnet_dummy_feature.shape}\")\n",
        "print(\"\\nThese are dummy features used to represent the output of MobileNetV2 and AlexNet.\")\n",
        "print(\"Actual image files are not available for feature extraction in this simulation.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of dummy MobileNetV2 features (image_features_train): (18783, 1280)\n",
            "Shape of dummy AlexNet feature for a single image: (1, 9216)\n",
            "\n",
            "These are dummy features used to represent the output of MobileNetV2 and AlexNet.\n",
            "Actual image files are not available for feature extraction in this simulation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ff81d4"
      },
      "source": [
        "## Prepare Training Data\n",
        "\n",
        "### Subtask:\n",
        "Prepare `image_features_train`, `input_sequences_train`, and `target_sequences_train` arrays by associating MobileNet image features with processed caption sequences, padding the caption sequences to `max_caption_length`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4aeacdb"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to initialize empty lists, create a mapping from image ID to index for feature retrieval, and then iterate through each image's captions to clean, tokenize, pad, and split them into input and target sequences. Finally, I will convert the lists to NumPy arrays and print their shapes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c442265b",
        "outputId": "96f9573b-2f48-4305-8328-9ac83c9b703b"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import string\n",
        "\n",
        "# 1. Initialize empty lists\n",
        "final_image_features = []\n",
        "input_sequences_train_list = []\n",
        "target_sequences_train_list = []\n",
        "\n",
        "# 2. Create image_id_to_idx mapping\n",
        "# Assuming image_features_train is ordered by the keys of image_captions.keys()\n",
        "# It is important to match the order of image_features_train with image_captions\n",
        "# The current `image_features_train` was created based on `num_unique_images = len(image_captions)`\n",
        "# The order of `image_captions.keys()` defines the index for `image_features_train`.\n",
        "image_id_to_idx = {img_id: i for i, img_id in enumerate(image_captions.keys())}\n",
        "\n",
        "# 3. Iterate through each image_id and its list of captions\n",
        "for image_id, captions in image_captions.items():\n",
        "    # a. Retrieve the corresponding image feature\n",
        "    img_feature_idx = image_id_to_idx[image_id]\n",
        "    current_image_feature = image_features_train[img_feature_idx]\n",
        "\n",
        "    for caption_text in captions:\n",
        "        # b.i. Clean the caption_text\n",
        "        caption_text = caption_text.lower()\n",
        "        caption_text = caption_text.translate(str.maketrans('', '', string.punctuation))\n",
        "        caption_text = ''.join([char for char in caption_text if not char.isdigit()])\n",
        "        caption_text = ' '.join(caption_text.split())\n",
        "        cleaned_caption = f\"startseq {caption_text} endseq\"\n",
        "\n",
        "        # b.ii. Convert to numerical sequence\n",
        "        numerical_sequence = tokenizer.texts_to_sequences([cleaned_caption])[0]\n",
        "\n",
        "        # b.iv. Pad the numerical sequence\n",
        "        padded_sequence = pad_sequences([numerical_sequence], maxlen=max_caption_length, padding='post')[0]\n",
        "\n",
        "        # b.v. Create input_sequence\n",
        "        input_sequence = padded_sequence[:-1]\n",
        "\n",
        "        # b.vi. Create target_sequence\n",
        "        target_sequence = padded_sequence[1:]\n",
        "\n",
        "        # b.vii. Append the retrieved image feature\n",
        "        final_image_features.append(current_image_feature)\n",
        "\n",
        "        # b.viii. Append the input_sequence\n",
        "        input_sequences_train_list.append(input_sequence)\n",
        "\n",
        "        # b.ix. Append the target_sequence\n",
        "        target_sequences_train_list.append(target_sequence)\n",
        "\n",
        "# 4. Convert lists into NumPy arrays\n",
        "image_features_train = np.array(final_image_features)\n",
        "input_sequences_train = np.array(input_sequences_train_list)\n",
        "target_sequences_train = np.array(target_sequences_train_list)\n",
        "\n",
        "# 5. Print the shapes\n",
        "print(f\"Shape of final image_features_train: {image_features_train.shape}\")\n",
        "print(f\"Shape of input_sequences_train: {input_sequences_train.shape}\")\n",
        "print(f\"Shape of target_sequences_train: {target_sequences_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of final image_features_train: (93950, 1280)\n",
            "Shape of input_sequences_train: (93950, 50)\n",
            "Shape of target_sequences_train: (93950, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "438d6d35"
      },
      "source": [
        "## Train GRU Model with Standard Embeddings\n",
        "\n",
        "### Subtask:\n",
        "Define and compile a GRU model using a standard Keras Embedding layer, integrating the MobileNet image features and the caption sequences. Train this model on the prepared training data. Record the total training time and plot the training loss curve over epochs to assess its performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef75681c"
      },
      "source": [
        "### Define and Compile the GRU Model\n",
        "\n",
        "**Reasoning**:\n",
        "I will import the necessary layers and modules from `tensorflow.keras` to build the GRU model. These include `Input`, `Embedding`, `GRU`, `Dense`, `add` from `tensorflow.keras.layers`, and `Model` from `tensorflow.keras.models`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "3a1c4d30",
        "outputId": "0004d901-05a4-4200-c45e-7f12b947a029"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Ensure image_feature_input_shape is correctly set to mobilenet_feature_shape\n",
        "# which is 1280 based on kernel state.\n",
        "# Ensure caption_input_length is max_caption_length - 1 which is 50.\n",
        "# Ensure vocab_size is 12844 and embedding_dim is 256 from kernel state.\n",
        "\n",
        "# Image input branch (MobileNet features)\n",
        "image_input = Input(shape=(mobilenet_feature_shape,), name='image_input')\n",
        "image_feature_layer = Dense(embedding_dim, activation='relu', name='image_feature_projection')(image_input)\n",
        "\n",
        "# Caption input branch\n",
        "caption_input = Input(shape=(caption_input_length,), name='caption_input')\n",
        "caption_embedding_layer = Embedding(vocab_size, embedding_dim, mask_zero=True, name='caption_embedding')(caption_input)\n",
        "caption_gru_layer = GRU(embedding_dim, return_sequences=True, name='caption_gru')(caption_embedding_layer)\n",
        "\n",
        "# Merge image and caption features\n",
        "merged_features = add([image_feature_layer, caption_gru_layer], name='merged_features')\n",
        "\n",
        "# Output layer\n",
        "output_layer = Dense(vocab_size, activation='softmax', name='output_prediction')(merged_features)\n",
        "\n",
        "# Create the full model\n",
        "model = Model(inputs=[image_input, caption_input], outputs=output_layer)\n",
        "\n",
        "print(\"Model created successfully with MobileNet input shape.\")\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model created successfully with MobileNet input shape.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ caption_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ caption_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │  \u001b[38;5;34m3,288,064\u001b[0m │ caption_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ caption_input[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_feature_proj… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m327,936\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ caption_gru (\u001b[38;5;33mGRU\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m394,752\u001b[0m │ caption_embeddin… │\n",
              "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ merged_features     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ image_feature_pr… │\n",
              "│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ caption_gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_prediction   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m12844\u001b[0m) │  \u001b[38;5;34m3,300,908\u001b[0m │ merged_features[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ caption_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ caption_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,288,064</span> │ caption_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ caption_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ image_feature_proj… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ caption_gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">394,752</span> │ caption_embeddin… │\n",
              "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ merged_features     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ image_feature_pr… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ caption_gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_prediction   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12844</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,300,908</span> │ merged_features[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,311,660\u001b[0m (27.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,311,660</span> (27.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,311,660\u001b[0m (27.89 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,311,660</span> (27.89 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d11b6f2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will import the necessary Keras layers and modules to define the GRU model, as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "496bb580",
        "outputId": "71f4c003-b53b-41c4-c8da-e49d755dcc56"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, GRU, Dense, add\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "print(\"Keras layers and Model imported successfully.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras layers and Model imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a904e012"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the Keras modules are imported, I will define the input shapes for the image features (MobileNet output dimension) and the caption sequences (maximum caption length - 1) as instructed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4393401",
        "outputId": "a0e6c207-6332-4f91-b650-b4150aed67b1"
      },
      "source": [
        "image_feature_input_shape = mobilenet_feature_shape # From kernel state, this is 1280\n",
        "caption_input_length = max_caption_length - 1 # Since target sequence is max_caption_length - 1\n",
        "\n",
        "print(f\"Image feature input shape: {image_feature_input_shape}\")\n",
        "print(f\"Caption input length: {caption_input_length}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image feature input shape: 1280\n",
            "Caption input length: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ab5be2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will create the image feature input branch of the model by defining an `Input` layer for the image features and then adding a `Dense` layer to project these features to the `embedding_dim` for consistent dimensionality with the caption embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41bc92b5",
        "outputId": "b71b82d9-e947-4893-c4c8-3e9396b1f58a"
      },
      "source": [
        "image_input = Input(shape=(image_feature_input_shape,), name='image_input')\n",
        "image_feature_layer = Dense(embedding_dim, activation='relu', name='image_feature_projection')(image_input)\n",
        "\n",
        "print(f\"Image input layer created with shape: {image_input.shape}\")\n",
        "print(f\"Image feature projection layer created with output shape: {image_feature_layer.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image input layer created with shape: (None, 1280)\n",
            "Image feature projection layer created with output shape: (None, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6398c1c"
      },
      "source": [
        "**Reasoning**:\n",
        "Now, I will create the caption input branch of the model. This involves defining an `Input` layer for the caption sequences, followed by an `Embedding` layer to convert numerical word indices into dense vectors, and then a `GRU` layer to process these sequences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01d15249",
        "outputId": "23d1f16d-bd4e-49ef-f7f7-ad80e0bcc602"
      },
      "source": [
        "caption_input = Input(shape=(caption_input_length,), name='caption_input')\n",
        "caption_embedding_layer = Embedding(vocab_size, embedding_dim, mask_zero=True, name='caption_embedding')(caption_input)\n",
        "caption_gru_layer = GRU(embedding_dim, return_sequences=True, name='caption_gru')(caption_embedding_layer)\n",
        "\n",
        "print(f\"Caption input layer created with shape: {caption_input.shape}\")\n",
        "print(f\"Caption embedding layer created with output shape: {caption_embedding_layer.shape}\")\n",
        "print(f\"Caption GRU layer created with output shape: {caption_gru_layer.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caption input layer created with shape: (None, 50)\n",
            "Caption embedding layer created with output shape: (None, 50, 256)\n",
            "Caption GRU layer created with output shape: (None, 50, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc500f8"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that both the image feature branch and the caption branch are defined, I will merge them using an `add` layer. After merging, a final `Dense` layer with `vocab_size` units and a 'softmax' activation will be added to produce the output probabilities for each word in the vocabulary, as specified in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33779f30",
        "outputId": "075a41ba-dc73-4a7a-ae2f-ebf346bd6de2"
      },
      "source": [
        "merged_features = add([image_feature_layer, caption_gru_layer], name='merged_features')\n",
        "output_layer = Dense(vocab_size, activation='softmax', name='output_prediction')(merged_features)\n",
        "\n",
        "print(f\"Merged features layer created with output shape: {merged_features.shape}\")\n",
        "print(f\"Output prediction layer created with output shape: {output_layer.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged features layer created with output shape: (None, 50, 256)\n",
            "Output prediction layer created with output shape: (None, 50, 12844)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a13581e4"
      },
      "source": [
        "# Task\n",
        "Create and summarize the GRU Model by creating a Keras `Model` instance using the defined `image_input`, `caption_input`, and `output_layer`, then print the `model.summary()` to display its architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c508cace"
      },
      "source": [
        "## Create and Summarize GRU Model\n",
        "\n",
        "### Subtask:\n",
        "Create the Keras `Model` instance using the defined input and output layers, then print the `model.summary()` to display the full architecture, layer by layer, including input/output shapes and parameter counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "dc96f0ca",
        "outputId": "703a6ee7-377a-4904-ae4e-65f50d775bc1"
      },
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "print(\"Model compiled successfully.\")\n",
        "\n",
        "# Define training parameters\n",
        "epochs = 5 # You can adjust this\n",
        "batch_size = 32 # You can adjust this\n",
        "\n",
        "# Record the start time before training\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    {'image_input': image_features_train, 'caption_input': input_sequences_train}, # Corrected key to match model's Input layer name\n",
        "    target_sequences_train,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1 # Show progress\n",
        ")\n",
        "\n",
        "# Record the end time and print total training duration\n",
        "end_time = time.time()\n",
        "training_duration = end_time - start_time\n",
        "print(f\"\\nTotal training duration: {training_duration:.2f} seconds\")\n",
        "\n",
        "# Plot the training loss over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.title('GRU Model Training Loss Over Epochs (Standard Embeddings)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model compiled successfully.\n",
            "Epoch 1/5\n",
            "\u001b[1m2936/2936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 54ms/step - accuracy: 0.8333 - loss: 1.1468\n",
            "Epoch 2/5\n",
            "\u001b[1m2936/2936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 54ms/step - accuracy: 0.8625 - loss: 0.7418\n",
            "Epoch 3/5\n",
            "\u001b[1m2936/2936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 54ms/step - accuracy: 0.8691 - loss: 0.6700\n",
            "Epoch 4/5\n",
            "\u001b[1m2936/2936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 54ms/step - accuracy: 0.8727 - loss: 0.6272\n",
            "Epoch 5/5\n",
            "\u001b[1m2936/2936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 55ms/step - accuracy: 0.8758 - loss: 0.5953\n",
            "\n",
            "Total training duration: 843.93 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4855148b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A Keras `Model` instance was successfully created, integrating `image_input`, `caption_input`, and an `output_layer` to define the model's architecture.\n",
        "*   The `model.summary()` function was used to display a detailed overview of the created GRU model, including layer names, layer types, output shapes, and parameter counts for each layer.\n",
        "*   The summary also provided a breakdown of the total, trainable, and non-trainable parameters within the entire model.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The model summary is essential for verifying the correct construction of the neural network and understanding its computational complexity before training.\n",
        "*   The next logical step would be to compile the model with an optimizer, loss function, and metrics, followed by training it with suitable image and caption data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download GloVe 300d (800 MB) — run once\n",
        "!wget -q http://nlp.stanford.edu/data/glove.6B.zip -O glove.zip\n",
        "!unzip -q glove.zip\n",
        "\n",
        "GLOVE_PATH = \"glove.6B.300d.txt\"\n"
      ],
      "metadata": {
        "id": "DZs-oLoij9HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_dim = 300\n",
        "embedding_index = {}\n",
        "\n",
        "print(\"Loading GloVe vectors...\")\n",
        "with open(GLOVE_PATH, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = vector\n",
        "\n",
        "print(\"Total GloVe words:\", len(embedding_index))\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < vocab_size:\n",
        "        vec = embedding_index.get(word)\n",
        "        if vec is not None:\n",
        "            embedding_matrix[i] = vec\n",
        "        else:\n",
        "            embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6W1S-eUj_QD",
        "outputId": "d8bcd7b8-3579-4ef5-aa95-992048c9d41f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe vectors...\n",
            "Total GloVe words: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Automatic fixer cell ----------\n",
        "import os, glob, time, json, numpy as np\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input as mn_preprocess\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# 0) Quick checks for already existing objects (avoid overwriting)\n",
        "have_tokenizer = 'tokenizer' in globals()\n",
        "have_embedding = 'embedding_matrix' in globals()\n",
        "have_vocab = 'vocab_size' in globals()\n",
        "\n",
        "print(\"Have tokenizer?\", have_tokenizer)\n",
        "print(\"Have embedding_matrix?\", have_embedding)\n",
        "print(\"vocab_size:\", globals().get('vocab_size', None))\n",
        "\n",
        "# 1) Try to find an existing features dict in memory or saved on disk\n",
        "features_dict = globals().get('features_dict', None)\n",
        "if features_dict is not None:\n",
        "    print(\"Using features_dict from memory, entries:\", len(features_dict))\n",
        "else:\n",
        "    # search for saved npz files in common locations\n",
        "    candidate_npzs = []\n",
        "    for root in ['/content', '/content/mini_coco', '/content/drive/MyDrive', '/content/drive/MyDrive/Datasets', '/content/data']:\n",
        "        if os.path.isdir(root):\n",
        "            candidate_npzs += glob.glob(os.path.join(root, '**', '*features*.npz'), recursive=True)\n",
        "            candidate_npzs += glob.glob(os.path.join(root, '**', '*.npz'), recursive=True)\n",
        "    candidate_npzs = [p for p in candidate_npzs if 'feature' in os.path.basename(p).lower() or 'feat' in os.path.basename(p).lower()]\n",
        "    candidate_npzs = sorted(set(candidate_npzs))\n",
        "    if candidate_npzs:\n",
        "        # pick the first and try to load\n",
        "        print(\"Found candidate .npz files (trying first):\", candidate_npzs[:3])\n",
        "        try:\n",
        "            data = np.load(candidate_npzs[0], allow_pickle=True)\n",
        "            # data.files gives keys; create dict mapping\n",
        "            features_dict = {k: data[k] for k in data.files}\n",
        "            print(\"Loaded features from\", candidate_npzs[0], \"count:\", len(features_dict))\n",
        "        except Exception as e:\n",
        "            print(\"Failed loading .npz:\", e)\n",
        "            features_dict = None\n",
        "    else:\n",
        "        print(\"No saved .npz features found.\")\n",
        "\n",
        "# 2) If still no features, try to locate image folder and extract features with MobileNetV2\n",
        "if features_dict is None:\n",
        "    print(\"Searching for image folders (common paths)...\")\n",
        "    search_paths = [\n",
        "        \"/content/mini_coco/images\",\n",
        "        \"/content/mini_coco\",\n",
        "        \"/content/drive/MyDrive/mini_coco/images\",\n",
        "        \"/content/drive/MyDrive/mini_coco\",\n",
        "        \"/content/drive/MyDrive/Datasets/mini_coco/images\",\n",
        "        \"/content/drive/MyDrive/Datasets/mini_coco\",\n",
        "        \"/content/drive/MyDrive\",\n",
        "        \"/content\"\n",
        "    ]\n",
        "    images_dir = None\n",
        "    for p in search_paths:\n",
        "        if os.path.isdir(p):\n",
        "            # look for many image files inside\n",
        "            imgs = glob.glob(os.path.join(p, '**', '*.jpg'), recursive=True) + glob.glob(os.path.join(p, '**', '*.png'), recursive=True)\n",
        "            if len(imgs) >= 10:\n",
        "                images_dir = p\n",
        "                break\n",
        "            # if few images but a subfolder 'images' exists, check that\n",
        "            imgs2 = glob.glob(os.path.join(p, 'images', '*.jpg')) + glob.glob(os.path.join(p, 'images', '*.png'))\n",
        "            if len(imgs2) >= 10:\n",
        "                images_dir = os.path.join(p, 'images')\n",
        "                break\n",
        "    if images_dir is None:\n",
        "        # fallback: list top-level of Drive to help user\n",
        "        print(\"Could not automatically locate an image folder with >=10 images.\")\n",
        "        print(\"Please set IMAGES_DIR to the folder containing your dataset images (e.g. '/content/mini_coco/images' or '/content/drive/MyDrive/mini_coco/images').\")\n",
        "    else:\n",
        "        print(\"Using images from:\", images_dir)\n",
        "        # Build MobileNetV2 extractor\n",
        "        base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "        gap = GlobalAveragePooling2D()(base.output)\n",
        "        extractor = Model(inputs=base.input, outputs=gap)\n",
        "        print(\"MobileNetV2 extractor ready. Output dim:\", extractor.output_shape[-1])\n",
        "\n",
        "        # get list of image paths (limit to reasonable number for speed; adjust as needed)\n",
        "        img_paths = glob.glob(os.path.join(images_dir, '**', '*.jpg'), recursive=True) + glob.glob(os.path.join(images_dir, '**', '*.png'), recursive=True)\n",
        "        img_paths = sorted(img_paths)\n",
        "        n_to_process = min(len(img_paths), 1000)   # change limit if you want full extraction\n",
        "        print(\"Found\", len(img_paths), \"images — extracting features for\", n_to_process, \"images (you can increase this).\")\n",
        "\n",
        "        features_dict = {}\n",
        "        t0 = time.time()\n",
        "        for p in img_paths[:n_to_process]:\n",
        "            try:\n",
        "                img = load_img(p, target_size=(224,224))\n",
        "                arr = img_to_array(img)\n",
        "                arr = np.expand_dims(arr, 0)\n",
        "                arr = mn_preprocess(arr)\n",
        "                feat = extractor.predict(arr, verbose=0)\n",
        "                key = os.path.basename(p)\n",
        "                features_dict[key] = feat.flatten()\n",
        "            except Exception as e:\n",
        "                print(\"skip\", p, \"error:\", e)\n",
        "        t1 = time.time()\n",
        "        print(\"Extraction finished in {:.1f}s — features stored for {} images\".format(t1-t0, len(features_dict)))\n",
        "        # optional: save for reuse\n",
        "        save_dir = \"/content/features_cache\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        np.savez_compressed(os.path.join(save_dir, \"mobilenet_features.npz\"), **features_dict)\n",
        "        print(\"Saved features to\", os.path.join(save_dir, \"mobilenet_features.npz\"))\n",
        "\n",
        "# 3) Compute feature_dim from features_dict\n",
        "if features_dict is not None:\n",
        "    first_feature = next(iter(features_dict.values()))\n",
        "    feature_dim = first_feature.shape[0]\n",
        "    print(\"feature_dim set to\", feature_dim)\n",
        "else:\n",
        "    feature_dim = None\n",
        "    print(\"feature_dim could not be set (no features found).\")\n",
        "\n",
        "# 4) Compute max_length:\n",
        "max_length = globals().get('max_length', None)\n",
        "if max_length is not None:\n",
        "    print(\"max_length already defined:\", max_length)\n",
        "else:\n",
        "    # Try to compute from available caption variables (common names)\n",
        "    candidate_captions_lists = []\n",
        "    for name in ['all_captions', 'captions', 'captions_map', 'all_texts']:\n",
        "        if name in globals():\n",
        "            candidate_captions_lists.append((name, globals()[name]))\n",
        "    # Also try common captions files in /content/mini_coco or drive\n",
        "    caption_files = []\n",
        "    for root in ['/content/mini_coco', '/content', '/content/drive/MyDrive', '/content/drive/MyDrive/Datasets']:\n",
        "        for fname in ['captions.txt', 'captions.json', 'captions.json.zip', 'captions.csv', 'annotations.json', 'captions_train2014.json']:\n",
        "            fpath = os.path.join(root, fname)\n",
        "            if os.path.exists(fpath):\n",
        "                caption_files.append(fpath)\n",
        "    if candidate_captions_lists:\n",
        "        # assume list of caption strings\n",
        "        cand_name, cand = candidate_captions_lists[0]\n",
        "        print(\"Computing max_length from variable:\", cand_name)\n",
        "        # flatten to strings\n",
        "        flat = []\n",
        "        if isinstance(cand, dict):\n",
        "            for v in cand.values():\n",
        "                if isinstance(v, list):\n",
        "                    flat += v\n",
        "                elif isinstance(v, str):\n",
        "                    flat.append(v)\n",
        "        elif isinstance(cand, list):\n",
        "            flat = cand\n",
        "        # sanitize\n",
        "        flat = [s for s in flat if isinstance(s, str)]\n",
        "        if flat:\n",
        "            max_length = max(len(s.split()) for s in flat)\n",
        "            print(\"Computed max_length =\", max_length, \"from\", cand_name)\n",
        "    elif caption_files:\n",
        "        # attempt to load a small caption file to compute max length\n",
        "        print(\"Found caption file(s). Trying to read\", caption_files[0])\n",
        "        try:\n",
        "            p = caption_files[0]\n",
        "            if p.endswith('.json') or p.endswith('.json.zip'):\n",
        "                # try to load JSON\n",
        "                if p.endswith('.zip'):\n",
        "                    import zipfile\n",
        "                    with zipfile.ZipFile(p, 'r') as z:\n",
        "                        names = z.namelist()\n",
        "                        # look for .json inside\n",
        "                        json_name = next((n for n in names if n.endswith('.json')), None)\n",
        "                        if json_name:\n",
        "                            with z.open(json_name) as jf:\n",
        "                                data = json.load(jf)\n",
        "                        else:\n",
        "                            data = None\n",
        "                else:\n",
        "                    with open(p,'r',encoding='utf-8') as f:\n",
        "                        data = json.load(f)\n",
        "                # try to extract captions from typical COCO style\n",
        "                caps = []\n",
        "                if isinstance(data, dict):\n",
        "                    # try 'annotations' key\n",
        "                    if 'annotations' in data:\n",
        "                        for ann in data['annotations']:\n",
        "                            if 'caption' in ann:\n",
        "                                caps.append(ann['caption'])\n",
        "                    else:\n",
        "                        # if list of objects with 'caption' field\n",
        "                        for item in data.get('images', []):\n",
        "                            pass\n",
        "                if caps:\n",
        "                    max_length = max(len(s.split()) for s in caps)\n",
        "                    print(\"Computed max_length from json file:\", max_length)\n",
        "            else:\n",
        "                # plain txt: each line a caption\n",
        "                with open(p, 'r', encoding='utf-8') as f:\n",
        "                    lines = [l.strip() for l in f if l.strip()]\n",
        "                if lines:\n",
        "                    max_length = max(len(l.split()) for l in lines)\n",
        "                    print(\"Computed max_length from text file:\", max_length)\n",
        "        except Exception as e:\n",
        "            print(\"Error reading caption file:\", e)\n",
        "    else:\n",
        "        print(\"Could not find captions in memory or common files. Using default max_length = 34 (typical for COCO).\")\n",
        "        max_length = 34\n",
        "\n",
        "# 5) Final sanity check and build model if possible\n",
        "print(\"\\nFinal status:\")\n",
        "print(\"feature_dim =\", feature_dim)\n",
        "print(\"max_length =\", max_length)\n",
        "print(\"vocab_size =\", globals().get('vocab_size', None))\n",
        "print(\"embedding_matrix shape =\", globals().get('embedding_matrix').shape if 'embedding_matrix' in globals() else None)\n",
        "\n",
        "if feature_dim is not None and max_length is not None and have_vocab and have_embedding:\n",
        "    # build model (re-define builder if not present)\n",
        "    from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, add\n",
        "    from tensorflow.keras.models import Model\n",
        "\n",
        "    def build_gru_glove(feature_dim, max_length, vocab_size, embedding_matrix):\n",
        "        img_input = Input(shape=(feature_dim,))\n",
        "        img_dropout = Dropout(0.5)(img_input)\n",
        "        img_dense = Dense(256, activation='relu')(img_dropout)\n",
        "\n",
        "        cap_input = Input(shape=(max_length,))\n",
        "        cap_embed = Embedding(vocab_size,\n",
        "                              embedding_matrix.shape[1],\n",
        "                              weights=[embedding_matrix],\n",
        "                              mask_zero=True,\n",
        "                              trainable=False)(cap_input)\n",
        "        cap_dropout = Dropout(0.5)(cap_embed)\n",
        "        cap_gru = GRU(256)(cap_dropout)\n",
        "\n",
        "        merged = add([img_dense, cap_gru])\n",
        "        dense1 = Dense(256, activation='relu')(merged)\n",
        "        output = Dense(vocab_size, activation='softmax')(dense1)\n",
        "\n",
        "        model = Model(inputs=[img_input, cap_input], outputs=output)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "        return model\n",
        "\n",
        "    gru_glove_model = build_gru_glove(feature_dim, max_length, vocab_size, embedding_matrix)\n",
        "    print(\"\\nModel built successfully. Summary:\")\n",
        "    gru_glove_model.summary()\n",
        "else:\n",
        "    print(\"\\nMissing required items; cannot build model automatically. If you want help, run `!ls` in /content and /content/mini_coco and tell me where images/captions are, or mount drive and place images in a folder and rerun this cell.\")\n",
        "# ---------- end fixer cell ----------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "fR5u_0_FlptM",
        "outputId": "2aef6fd7-3c27-4ad3-cbaf-d8dc892018b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Have tokenizer? True\n",
            "Have embedding_matrix? True\n",
            "vocab_size: 12844\n",
            "No saved .npz features found.\n",
            "Searching for image folders (common paths)...\n",
            "Using images from: /content/drive/MyDrive\n",
            "MobileNetV2 extractor ready. Output dim: 1280\n",
            "Found 302 images — extracting features for 302 images (you can increase this).\n",
            "Extraction finished in 42.9s — features stored for 302 images\n",
            "Saved features to /content/features_cache/mobilenet_features.npz\n",
            "feature_dim set to 1280\n",
            "Computing max_length from variable: captions\n",
            "Computed max_length = 12 from captions\n",
            "\n",
            "Final status:\n",
            "feature_dim = 1280\n",
            "max_length = 12\n",
            "vocab_size = 12844\n",
            "embedding_matrix shape = (12844, 300)\n",
            "\n",
            "Model built successfully. Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │  \u001b[38;5;34m3,853,200\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ input_layer_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_layer_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m327,936\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_1 (\u001b[38;5;33mGRU\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m428,544\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m65,792\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12844\u001b[0m)     │  \u001b[38;5;34m3,300,908\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ input_layer_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,853,200</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">428,544</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12844</span>)     │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,300,908</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,976,380\u001b[0m (30.43 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,976,380</span> (30.43 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,123,180\u001b[0m (15.73 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,123,180</span> (15.73 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,853,200\u001b[0m (14.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,853,200</span> (14.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# Example minimal generator\n",
        "def data_generator(features_dict, captions_dict, tokenizer, max_length, vocab_size, batch_size=32):\n",
        "    keys = list(captions_dict.keys())\n",
        "    n = len(keys)\n",
        "    while True:\n",
        "        X1, X2, y = [], [], []\n",
        "        for i, key in enumerate(keys):\n",
        "            img_feat = features_dict[key]\n",
        "            cap = captions_dict[key]\n",
        "            seq = tokenizer.texts_to_sequences([cap])[0]\n",
        "            for j in range(1, len(seq)):\n",
        "                in_seq, out_seq = seq[:j], seq[j]\n",
        "                in_seq = np.pad(in_seq, (0, max_length - len(in_seq)))\n",
        "                out_seq = to_categorical(out_seq, num_classes=vocab_size)\n",
        "                X1.append(img_feat)\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "            if len(X1) >= batch_size:\n",
        "                yield [np.array(X1), np.array(X2)], np.array(y)\n",
        "                X1, X2, y = [], [], []\n"
      ],
      "metadata": {
        "id": "_7Q3Lu-sooot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_train = {}\n",
        "\n",
        "# Case 1: COCO-style dict\n",
        "if isinstance(data, dict) and 'annotations' in data:\n",
        "    for ann in data['annotations']:\n",
        "        image_id = ann['image_id']\n",
        "        caption = ann['caption']\n",
        "        # Find image filename\n",
        "        image_info = next((img for img in data.get('images', []) if img['id'] == image_id), None)\n",
        "        if image_info:\n",
        "            file_name = image_info['file_name']\n",
        "            captions_train[file_name] = caption\n",
        "\n",
        "# Case 2: list of annotation dicts\n",
        "elif isinstance(data, list):\n",
        "    for ann in data:\n",
        "        if 'image_id' in ann and 'caption' in ann:\n",
        "            file_name = ann.get('file_name', f\"{ann['image_id']}.jpg\")  # fallback name\n",
        "            captions_train[file_name] = ann['caption']\n",
        "\n",
        "print(\"Number of captions loaded:\", len(captions_train))\n",
        "print(\"Sample entries:\", list(captions_train.items())[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4owHfPro9wj",
        "outputId": "4b2341b0-d687-4626-9745-23c3fd938c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of captions loaded: 18783\n",
            "Sample entries: [('299675.jpg', 'A white kitchen filled with pots, pans and dishes.'), ('513461.jpg', 'A family carrying a surfboard to the ocean.'), ('18691.jpg', 'A group of people are riding the bus.'), ('285579.jpg', 'A man walking in the rain crossing a street while holding an umbrella.'), ('266366.jpg', 'A bathroom with a toilet, sink, mirror and shower stall.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the basename as key\n",
        "features_dict = {os.path.basename(k): v for k, v in features_dict.items()}\n"
      ],
      "metadata": {
        "id": "Q_2bJUWepYVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_train = data_generator(features_dict, captions_train, tokenizer, max_length, vocab_size)\n"
      ],
      "metadata": {
        "id": "AKrN2KG2orWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(model, photo_feature):\n",
        "    in_text = \"startseq\"\n",
        "    for i in range(max_length):\n",
        "        seq = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        seq = pad_sequences([seq], max_length)\n",
        "        yhat = model.predict([photo_feature.reshape(1,-1), seq], verbose=0)\n",
        "        word = tokenizer.index_word.get(np.argmax(yhat), None)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n"
      ],
      "metadata": {
        "id": "fJ68exhYkGer"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}